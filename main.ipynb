{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fab040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import csv\n",
    "import copy\n",
    "import random \n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "import itertools\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mediapipe.tasks.python.components import containers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "68818eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options_hand = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options_hand = vision.HandLandmarkerOptions(base_options=base_options_hand,\n",
    "                                       num_hands=2,min_hand_detection_confidence=0.5,\n",
    "    min_hand_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    running_mode=vision.RunningMode.VIDEO)\n",
    "detector_hand = vision.HandLandmarker.create_from_options(options_hand)\n",
    "\n",
    "mp_hands = mp.tasks.vision.HandLandmarksConnections\n",
    "mp_drawing = mp.tasks.vision.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6e24dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Hand Landmarker (Existing) ---\n",
    "base_options_hand = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options_hand = vision.HandLandmarkerOptions(\n",
    "    base_options=base_options_hand,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.5,\n",
    "    min_hand_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    running_mode=vision.RunningMode.VIDEO\n",
    ")\n",
    "detector_hand = vision.HandLandmarker.create_from_options(options_hand)\n",
    "\n",
    "# # --- 2. Face Detector (BlazeFace) ---\n",
    "# # Finds where the face is so we can crop it\n",
    "# base_options_det = python.BaseOptions(model_asset_path='blaze_face_short_range.tflite')\n",
    "# options_det = vision.FaceDetectorOptions(\n",
    "#     base_options=base_options_det,\n",
    "#     running_mode=vision.RunningMode.IMAGE # We run this manually on frames\n",
    "# )\n",
    "# face_detector = vision.FaceDetector.create_from_options(options_det)\n",
    "\n",
    "# # --- 3. Image Embedder (MobileNet) ---\n",
    "# # Converts the cropped face image into a vector for recognition\n",
    "# base_options_emb = python.BaseOptions(model_asset_path='face_embedder.tflite')\n",
    "# options_emb = vision.ImageEmbedderOptions(\n",
    "#     base_options=base_options_emb,\n",
    "#     l2_normalize=True, # Critical for cosine similarity\n",
    "#     quantize=True,\n",
    "#     running_mode=vision.RunningMode.IMAGE\n",
    "# )\n",
    "# face_embedder = vision.ImageEmbedder.create_from_options(options_emb)\n",
    "\n",
    "mp_hands = mp.tasks.vision.HandLandmarksConnections\n",
    "mp_drawing = mp.tasks.vision.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a449a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustFaceManager:\n",
    "    def __init__(self, database_file='user_db.json'):\n",
    "        self.user_database = {} \n",
    "        self.similarity_threshold = 0.65 # Threshold\n",
    "        self.database_file = database_file\n",
    "        \n",
    "        # 1. Initialize Face Detector\n",
    "        base_options_det = python.BaseOptions(model_asset_path='blaze_face_short_range.tflite')\n",
    "        options_det = vision.FaceDetectorOptions(\n",
    "            base_options=base_options_det,\n",
    "            running_mode=vision.RunningMode.IMAGE\n",
    "        )\n",
    "        self.detector = vision.FaceDetector.create_from_options(options_det)\n",
    "\n",
    "        # 2. Initialize Image Embedder\n",
    "        base_options_emb = python.BaseOptions(model_asset_path='face_embedder.tflite')\n",
    "        options_emb = vision.ImageEmbedderOptions(\n",
    "            base_options=base_options_emb,\n",
    "            l2_normalize=True, \n",
    "            quantize=True,\n",
    "            running_mode=vision.RunningMode.IMAGE\n",
    "        )\n",
    "        self.embedder = vision.ImageEmbedder.create_from_options(options_emb)\n",
    "        \n",
    "        self.load_database()\n",
    "\n",
    "    def load_database(self):\n",
    "        if os.path.exists(self.database_file):\n",
    "            try:\n",
    "                with open(self.database_file, 'r') as f:\n",
    "                    raw_db = json.load(f)\n",
    "                \n",
    "                # --- CLEANING STEP ---\n",
    "                # Remove 'zero vectors' that cause divide-by-zero errors\n",
    "                self.user_database = {}\n",
    "                for name, vectors in raw_db.items():\n",
    "                    valid_vectors = []\n",
    "                    for v in vectors:\n",
    "                        # Convert to numpy and check if it has data (norm > 0)\n",
    "                        v_np = np.array(v, dtype=float)\n",
    "                        if np.linalg.norm(v_np) > 0.001: \n",
    "                            valid_vectors.append(v)\n",
    "                    \n",
    "                    if valid_vectors:\n",
    "                        self.user_database[name] = valid_vectors\n",
    "                        \n",
    "                print(f\"Loaded {len(self.user_database)} users (cleaned bad data).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Database error: {e}\")\n",
    "                self.user_database = {}\n",
    "\n",
    "    def save_database(self):\n",
    "        serializable_db = {}\n",
    "        for name, vectors in self.user_database.items():\n",
    "            # Ensure we save simple lists, not numpy arrays\n",
    "            serializable_db[name] = [v if isinstance(v, list) else v.tolist() for v in vectors]\n",
    "        \n",
    "        with open(self.database_file, 'w') as f:\n",
    "            json.dump(serializable_db, f)\n",
    "        print(\"Database saved.\")\n",
    "\n",
    "    def get_face_box(self, mp_image):\n",
    "        detection_result = self.detector.detect(mp_image)\n",
    "        if detection_result.detections:\n",
    "            return detection_result.detections[0].bounding_box\n",
    "        return None\n",
    "\n",
    "    def get_face_embedding(self, mp_image):\n",
    "        # A. DETECT\n",
    "        detection_result = self.detector.detect(mp_image)\n",
    "        if not detection_result.detections: return None \n",
    "\n",
    "        # B. CROP\n",
    "        detection = detection_result.detections[0]\n",
    "        bbox = detection.bounding_box\n",
    "        \n",
    "        image_np = mp_image.numpy_view()\n",
    "        h, w, c = image_np.shape\n",
    "        x, y = max(0, bbox.origin_x), max(0, bbox.origin_y)\n",
    "        w_box, h_box = min(w - x, bbox.width), min(h - y, bbox.height)\n",
    "\n",
    "        face_crop_np = image_np[y:y+h_box, x:x+w_box]\n",
    "        if face_crop_np.size == 0: return None\n",
    "\n",
    "        # C. EMBED\n",
    "        mp_face_crop = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.ascontiguousarray(face_crop_np))\n",
    "        embedding_result = self.embedder.embed(mp_face_crop)\n",
    "        \n",
    "        if embedding_result.embeddings:\n",
    "            return embedding_result.embeddings[0].embedding\n",
    "        return None\n",
    "\n",
    "    def _safe_cosine_similarity(self, v1, v2):\n",
    "        \"\"\"\n",
    "        Manually calculates similarity to avoid Divide-By-Zero crashes.\n",
    "        Returns 0.0 if vectors are invalid.\n",
    "        \"\"\"\n",
    "        v1 = np.array(v1, dtype=float)\n",
    "        v2 = np.array(v2, dtype=float)\n",
    "        \n",
    "        dot = np.dot(v1, v2)\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0 # Bad vector, return 0 similarity\n",
    "            \n",
    "        return dot / (norm1 * norm2)\n",
    "\n",
    "    def register_user_sample(self, name, mp_image):\n",
    "        embedding = self.get_face_embedding(mp_image)\n",
    "        if embedding is not None:\n",
    "            # Check norm before saving!\n",
    "            if np.linalg.norm(embedding) < 0.001: return False\n",
    "\n",
    "            if name not in self.user_database:\n",
    "                self.user_database[name] = []\n",
    "            \n",
    "            if isinstance(embedding, np.ndarray): embedding = embedding.tolist()\n",
    "            self.user_database[name].append(embedding)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def identify(self, mp_image):\n",
    "        unknown_vector = self.get_face_embedding(mp_image)\n",
    "        if unknown_vector is None: return \"Unknown\", 0.0\n",
    "\n",
    "        best_user = \"Unknown\"\n",
    "        highest_similarity = -1\n",
    "\n",
    "        for name, saved_list in self.user_database.items():\n",
    "            for saved_vector in saved_list:\n",
    "                # USE SAFE SIMILARITY\n",
    "                sim = self._safe_cosine_similarity(unknown_vector, saved_vector)\n",
    "                \n",
    "                if sim > highest_similarity:\n",
    "                    highest_similarity = sim\n",
    "                    best_user = name\n",
    "\n",
    "        if highest_similarity > self.similarity_threshold:\n",
    "            return best_user, highest_similarity\n",
    "        else:\n",
    "            return \"Unknown\", highest_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "870500b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_manager = RobustFaceManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "75383237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enroll_new_user(user_name, capture_duration=10):\n",
    "    print(f\"Starting enrollment for {user_name}. Please move your head slowly...\")\n",
    "    \n",
    "    collected_embeddings = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < capture_duration:\n",
    "        success, frame = cap.read()\n",
    "        if not success: break\n",
    "        \n",
    "        # 1. Get Embedding\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        timestamp = int(time.time() * 1000)\n",
    "        result = face_embedder.embed(mp_image) # Using non-video mode for simplicity here, or use embed_for_video\n",
    "        \n",
    "        if result.embeddings:\n",
    "            # Save the embedding object (specifically the numpy array part)\n",
    "            # MediaPipe returns a float array, convert to list or keep as numpy\n",
    "            vector = result.embeddings[0].embedding\n",
    "            collected_embeddings.append(vector)\n",
    "            \n",
    "        cv.imshow(f\"Enrolling {user_name}\", frame)\n",
    "        cv.waitKey(5)\n",
    "        \n",
    "    # 2. Save to Manager\n",
    "    face_manager.register_user_samples(user_name, collected_embeddings)\n",
    "    print(f\"Enrollment complete! Captured {len(collected_embeddings)} angles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f6fb339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result,prediction,confidence,brect):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    mp_drawing.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks,\n",
    "      mp_hands.HAND_CONNECTIONS)\n",
    "    if len(brect) > 0:      \n",
    "      cv.putText(annotated_image, f\"{prediction}({confidence:.3f})\",(brect[0],brect[1]-5),cv.FONT_HERSHEY_PLAIN,1,(0,0,0),1)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8b35ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(image):\n",
    "    return image.hand_landmarks #list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2ab242ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(hand_landmarks):\n",
    "    temp_landmark_list = copy.deepcopy(hand_landmarks)\n",
    "\n",
    "    base_x, base_y = 0, 0\n",
    "    points = []\n",
    "\n",
    "    for i,landmark in enumerate(temp_landmark_list):\n",
    "        px = landmark.x if hasattr(landmark, 'x') else landmark[0]\n",
    "        py = landmark.y if hasattr(landmark, 'y') else landmark[1]\n",
    "\n",
    "        points.append([px, py])\n",
    "\n",
    "        if i == 0:\n",
    "            base_x, base_y = px, py\n",
    "\n",
    "    for point in points:\n",
    "        point[0] = point[0] - base_x\n",
    "        point[1] = point[1] - base_y\n",
    "\n",
    "    max_value = max(list(map(abs, itertools.chain.from_iterable(points))))\n",
    "\n",
    "    if max_value == 0: \n",
    "        max_value = 1\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    for point in points:\n",
    "        point[0] = normalize_(point[0])\n",
    "        point[1] = normalize_(point[1])\n",
    "\n",
    "    flattened_list = list(itertools.chain.from_iterable(points))\n",
    "\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d397c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_landmarks(landmarks, handedness):\n",
    "    canonical_points = []\n",
    "    need_flip = handedness\n",
    "\n",
    "    for landmark in landmarks:\n",
    "        \n",
    "        x = landmark.x\n",
    "        y = landmark.y\n",
    "    \n",
    "        if need_flip:\n",
    "            x = 1.0 - x\n",
    "            \n",
    "        canonical_points.append([x, y])\n",
    "    return canonical_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4d3dc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_landmarks_list(landmark_list):\n",
    "    augmented_rows = []\n",
    "    augmented_rows.append(landmark_list)\n",
    "\n",
    "    for _ in range(4): \n",
    "        new_landmarks = []\n",
    "\n",
    "        theta = math.radians(random.uniform(-10, 10))\n",
    "        c, s = math.cos(theta), math.sin(theta)\n",
    "\n",
    "        scale = random.uniform(0.9, 1.1)\n",
    "\n",
    "        for i in range(0, len(landmark_list), 2):\n",
    "            x = landmark_list[i]\n",
    "            y = landmark_list[i+1]\n",
    "            \n",
    "            x_new = (x * c) - (y * s)\n",
    "            y_new = (x * s) + (y * c)\n",
    "            \n",
    "            x_new *= scale\n",
    "            y_new *= scale\n",
    "\n",
    "            new_landmarks.extend([x_new, y_new])\n",
    "            \n",
    "        augmented_rows.append(new_landmarks)\n",
    "        \n",
    "    return augmented_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8888bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(csv_path='gesture_data.csv', model_save_path='gesture_model.pkl'):\n",
    "    print(f\"Loading data from {csv_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: CSV file not found. Have you recorded any gestures yet?\")\n",
    "        return None\n",
    "\n",
    "    # Separate Features (X) and Labels (y)\n",
    "    X = df.iloc[:, 1:].values  \n",
    "    y = df.iloc[:, 0].values\n",
    "    \n",
    "    # --- CHECK: Do we have at least 2 classes? ---\n",
    "    unique_classes = np.unique(y)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"CANNOT TRAIN YET: Found only 1 class ({unique_classes[0]}).\")\n",
    "        print(\">>> PLEASE RECORD A 'NEUTRAL' GESTURE NEXT! <<<\")\n",
    "        return None \n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Stratify needs at least 2 samples per class. Safety check:\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Warning: Not enough data to split perfectly. Training on full dataset.\")\n",
    "        X_train, y_train = X, y\n",
    "        X_test, y_test = [], []\n",
    "\n",
    "    # FIX: Print statement is now AFTER X_train is defined\n",
    "    print(f\"Training on {len(X_train)} samples with classes: {unique_classes}\")\n",
    "\n",
    "    model = SVC(kernel='linear', probability=True) \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    with open(model_save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_save_path}!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "75ffc5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from gesture_data.csv...\n",
      "Error: CSV file not found. Have you recorded any gestures yet?\n"
     ]
    }
   ],
   "source": [
    "train_custom_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "311d43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    coords = np.array([[min(int(l.x * image_width), image_width - 1), \n",
    "                        min(int(l.y * image_height), image_height - 1)] \n",
    "                       for l in landmarks])\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(coords)\n",
    "\n",
    "    return [x, y, x + w, y + h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "68286e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "06399b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controls:\n",
      "  'e' -> Enroll a NEW USER\n",
      "  'r' -> Record a NEW GESTURE\n",
      "  's' -> STOP and SAVE\n",
      "  'q' -> Quit\n",
      "Flushing buffer...\n",
      "Database saved.\n",
      "User Saved.\n",
      "Flushing buffer...\n",
      "Loading data from Garv_data.csv...\n",
      "CANNOT TRAIN YET: Found only 1 class (open palm).\n",
      ">>> PLEASE RECORD A 'NEUTRAL' GESTURE NEXT! <<<\n",
      "Flushing buffer...\n",
      "Loading data from Garv_data.csv...\n",
      "Training on 2064 samples with classes: ['open palm' 'up']\n",
      "Model Accuracy: 98.84%\n",
      "Model saved to Garv_model.pkl!\n",
      "Flushing buffer...\n",
      "Loading data from Garv_data.csv...\n",
      "Training on 3200 samples with classes: ['nothing' 'open palm' 'up']\n",
      "Model Accuracy: 97.38%\n",
      "Model saved to Garv_model.pkl!\n",
      "Flushing buffer...\n",
      "Loading data from Garv_data.csv...\n",
      "Training on 4152 samples with classes: ['nothing' 'open palm' 'peace' 'up']\n",
      "Model Accuracy: 97.50%\n",
      "Model saved to Garv_model.pkl!\n",
      "Flushing buffer...\n",
      "Loading data from Garv_data.csv...\n",
      "Training on 4784 samples with classes: ['left' 'nothing' 'open palm' 'peace' 'up']\n",
      "Model Accuracy: 97.49%\n",
      "Model saved to Garv_model.pkl!\n",
      "User left. Resetting...\n"
     ]
    }
   ],
   "source": [
    "# --- INITIALIZATION ---\n",
    "cap = cv.VideoCapture(0)\n",
    "face_manager = RobustFaceManager() # Ensures DB is loaded and clean\n",
    "\n",
    "# State Variables\n",
    "current_user = None\n",
    "current_hand_model = None\n",
    "recording_mode = False      \n",
    "enrollment_mode = False     \n",
    "frames_recorded = 0\n",
    "frame_count = 0 \n",
    "missed_face_frames = 0 # <--- NEW: Memory Counter\n",
    "\n",
    "print(\"Controls:\")\n",
    "print(\"  'e' -> Enroll a NEW USER\")\n",
    "print(\"  'r' -> Record a NEW GESTURE\")\n",
    "print(\"  's' -> STOP and SAVE\")\n",
    "print(\"  'q' -> Quit\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success: break\n",
    "    frame_count += 1\n",
    "    \n",
    "    key = cv.waitKey(5) & 0xFF\n",
    "    if key == ord('q') or key == 27: break\n",
    "\n",
    "    # Prepare Image\n",
    "    debug_image = copy.deepcopy(img)\n",
    "    image_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "    frame_timestamp_ms = int(time.time() * 1000)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # PHASE 1: VISUALS & PERSISTENCE CHECK\n",
    "    # ----------------------------------------------------\n",
    "    face_bbox = face_manager.get_face_box(mp_image)\n",
    "    \n",
    "    if face_bbox:\n",
    "        # Face Found -> Reset counter\n",
    "        missed_face_frames = 0 \n",
    "        x, y, w, h = face_bbox.origin_x, face_bbox.origin_y, face_bbox.width, face_bbox.height\n",
    "        cv.rectangle(debug_image, (x, y), (x + w, y + h), (255, 255, 0), 2)\n",
    "    else:\n",
    "        # No Face -> Increment counter\n",
    "        missed_face_frames += 1\n",
    "        \n",
    "        # LOGIC: Only reset if we haven't seen a face for ~1 second (20 frames)\n",
    "        # AND we are NOT currently enrolling a user.\n",
    "        if missed_face_frames > 20 and not enrollment_mode:\n",
    "            if current_user is not None:\n",
    "                print(\"User left. Resetting...\")\n",
    "                current_user = None\n",
    "                current_hand_model = None\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # PHASE 2: RECOGNITION & ENROLLMENT\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    # A. ENROLLMENT (Always run if active, even if face momentarily lost)\n",
    "    if enrollment_mode and face_bbox:\n",
    "        success = face_manager.register_user_sample(current_user, mp_image)\n",
    "        if success:\n",
    "            cv.putText(debug_image, f\"ENROLLING {current_user}...\", (10, 50), \n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            count = len(face_manager.user_database.get(current_user, []))\n",
    "            cv.putText(debug_image, f\"Samples: {count}\", (10, 90), \n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # B. RECOGNITION (Run periodically if face is present)\n",
    "    elif face_bbox and (frame_count % 10 == 0) and not recording_mode:\n",
    "        user_name, score = face_manager.identify(mp_image)\n",
    "        \n",
    "        if user_name != \"Unknown\":\n",
    "            # Only load if it's a DIFFERENT user\n",
    "            if user_name != current_user:\n",
    "                print(f\"Switched User: {user_name} (Score: {score:.2f})\")\n",
    "                current_user = user_name\n",
    "                try:\n",
    "                    with open(f'{current_user}_model.pkl', 'rb') as f:\n",
    "                        current_hand_model = pickle.load(f)\n",
    "                    print(f\"Loaded gestures for {current_user}\")\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"No gestures found for {current_user}\")\n",
    "                    current_hand_model = None\n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "\n",
    "    # Visual Status\n",
    "    if current_user:\n",
    "        cv.putText(debug_image, f\"User: {current_user}\", (10, 30), \n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv.putText(debug_image, \"User: Unknown\", (10, 30), \n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # PHASE 3: HAND GESTURES\n",
    "    # ----------------------------------------------------\n",
    "    hand_result = detector_hand.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "    \n",
    "    if hand_result.hand_landmarks:\n",
    "        for i, hand_landmarks in enumerate(hand_result.hand_landmarks):\n",
    "            # Hand BBox\n",
    "            h_img, w_img, _ = debug_image.shape\n",
    "            x_vals = [lm.x for lm in hand_landmarks]\n",
    "            y_vals = [lm.y for lm in hand_landmarks]\n",
    "            brect = [int(min(x_vals)*w_img), int(min(y_vals)*h_img), int(max(x_vals)*w_img), int(max(y_vals)*h_img)]\n",
    "            \n",
    "            # Logic\n",
    "            handedness_obj = hand_result.handedness[i][0]\n",
    "            is_left = (handedness_obj.category_name == \"Left\") \n",
    "            canonical_points = get_canonical_landmarks(hand_landmarks, is_left) \n",
    "            normalized_flat = normalize(canonical_points)\n",
    "\n",
    "            # Record\n",
    "            if recording_mode and current_user:\n",
    "                batch = augment_landmarks_list(normalized_flat)\n",
    "                with open(f\"{current_user}_data.csv\", 'a', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    for row in batch:\n",
    "                        writer.writerow([current_label] + row)\n",
    "                frames_recorded += 1\n",
    "                cv.putText(debug_image, f\"REC: {current_label} ({frames_recorded})\", (10, 70), \n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Predict\n",
    "            elif current_hand_model:\n",
    "                prediction = current_hand_model.predict([normalized_flat])\n",
    "                confidence = current_hand_model.predict_proba([normalized_flat])\n",
    "                pred_label = prediction[0]\n",
    "                max_conf = np.max(confidence)\n",
    "\n",
    "                if max_conf > 0.6:\n",
    "                    cv.rectangle(debug_image, (brect[0], brect[1]), (brect[2], brect[3]), (0, 255, 0), 2)\n",
    "                    debug_image = draw_landmarks_on_image(debug_image, hand_result, pred_label, max_conf, brect)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # INPUT CONTROLS\n",
    "    # ----------------------------------------------------\n",
    "    if key == ord('e'): # Enroll\n",
    "        name = input(\"Enter Name: \")\n",
    "        print(\"Flushing buffer...\")\n",
    "        for _ in range(5): cap.read()\n",
    "        current_user = name\n",
    "        enrollment_mode = True\n",
    "        recording_mode = False\n",
    "        missed_face_frames = 0 # Reset counter so we don't logout immediately\n",
    "\n",
    "    elif key == ord('r'): # Record Gesture\n",
    "        if current_user:\n",
    "            current_label = input(\"Enter Gesture Name: \")\n",
    "            print(\"Flushing buffer...\")\n",
    "            for _ in range(5): cap.read()\n",
    "            recording_mode = True\n",
    "            enrollment_mode = False\n",
    "            frames_recorded = 0\n",
    "        else:\n",
    "            print(\"Enroll a user first!\")\n",
    "\n",
    "    elif key == ord('s'): # Save\n",
    "        if enrollment_mode:\n",
    "            face_manager.save_database()\n",
    "            enrollment_mode = False\n",
    "            print(\"User Saved.\")\n",
    "        elif recording_mode:\n",
    "            recording_mode = False\n",
    "            # Train\n",
    "            new_model = train_custom_model(csv_path=f\"{current_user}_data.csv\", \n",
    "                                           model_save_path=f\"{current_user}_model.pkl\")\n",
    "            if new_model: current_hand_model = new_model\n",
    "\n",
    "    cv.imshow(\"System\", debug_image)\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
